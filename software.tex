\section{Software}
\subsection{GPGPU \& CUDA}
\begin{frame}
  \frametitle{GPGPU \& CUDA}
     \begin{itemize}
        \item GPU (Graphic Processing Unit):\newline 
	      orginally developed for graphical applications.
        \item GP-GPU: General-Purpose GPU, i.e.\newline 
	      the use of GPUs beyond graphical applications.\newline
	      \textbf{\textcolor{red}{CAVEAT}}: problem to be reformulated in terms of the graphics API.
      \item \textbf{\textcolor{green}{2006}}: NVIDIA introduces the \textbf{\textcolor{blue}{CUDA}}\footnote{The \href{https://developer.nvidia.com/cuda-downloads}{CUDA Toolkit} consists of $2$ parts: 
      \begin{itemize} 
	      \item CUDA Driver 
	      \item CUDA Toolkit (\texttt{nvcc,nvprof}, \ldots, libraries, header files).
      \end{itemize} } framework\newline 
		(\textbf{\textcolor{blue}{C}}ompute \textbf{\textcolor{blue}{U}}nified 
		     \textbf{\textcolor{blue}{D}}evice \textbf{\textcolor{blue}{A}}rchitecture) 
              \begin{itemize}
	         \item CUDA API: extension of the \texttt{C} language.
                 \item handles the GPU thread level parallelism
                 \item deals with moving data between CPU and GPU.
		 \item also support for \CC\,, \texttt{Fortran}.			 
              \end{itemize}			 
     \end{itemize}		     
\end{frame}	

% Matrix Multiplication
%   consists of concepts of threads.
\subsection{Introducing CUDA concepts: case of matrix mul.}
% Step 1: consists of the concept of a kernel (__device__,__global__,..), threads
%         and how to transfer the data from the GPU and CPU. 
%         cudaMalloc, cudaMallocManaged (unified memory)
% Step 2: introduces the concept of the blocks and grids
%         
% Step 3: Introduces the concept of shared memory (+__syncthreads())

\subsection{Warps}
\begin{frame}
\frametitle{GPU Threads - Warps}
\begin{itemize}
  \item Each SMP:
  \begin{itemize}
     \item generates, schedules, executes threads in batches of $32$ threads.
     \item \textbf{WARP}: a batch of $32$ threads
  \end{itemize}
  \item each thread in a WARP executes the same instructions but runs its own "path".
  \item if threads within a WARP diverge, the threads become inactive/disabled.
\end{itemize}
\end{frame}


% Compilation of the CUDA code
%   introducing -compute, -arch
\subsection{Compiling CUDA code \& useful env. variables} 

%   nvprof, nvsight, cuda-gdb
\subsection{Debugging \& profiling your CUDA code}

% CUDA Libraries: CUBLAS, CUFFT, CUDNN, MAGMA, CURAND, NCCL
\subsection{Important CUDA Libraries}

% Alternatives:
%  - Within CUDA: CudaFortran, OpenAcc (pragmas)
%  - Alternatives to CUDA: OpenCL, HIP
%  - Kokkos
\subsection{Alternatives for CUDA}

\subsection{Links}
\begin{frame}
   \frametitle{Links}
      \begin{itemize}
	      \item \href{https://docs.nvidia.com/cuda/index.html}{CUDA Toolkit Documentation}
              \item \href{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html}{CUDA \CC\, Programming Guide}
	      \item \href{https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html}{CUDA \CC\, Best Practices Guide}	      
      \end{itemize}		      
\end{frame}	

